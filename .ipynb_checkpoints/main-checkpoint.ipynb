{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b7bd86",
   "metadata": {},
   "source": [
    "# Predicting Bank Campaign Success\n",
    "\n",
    "**Objective:** Build a binary classification model to predict whether a customer will subscribe to a term deposit (`y`) based on demographic, financial, and campaign-related features.\n",
    "\n",
    "**Dataset:** Bank Marketing Campaign (semicolon-separated CSV)\n",
    "\n",
    "**Target Variable:** `y` (yes/no) — Term deposit subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03cb970",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff5d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b52da1",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b367f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (semicolon-separated)\n",
    "df = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64023f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Duplicate Rows:\")\n",
    "print(f\"Number of duplicates: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d24104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle duplicates if any\n",
    "df_clean = df.drop_duplicates()\n",
    "print(f\"Shape after removing duplicates: {df_clean.shape}\")\n",
    "\n",
    "# Check for 'unknown' values in categorical columns\n",
    "categorical_cols = df_clean.select_dtypes(include='object').columns\n",
    "print(\"\\n'Unknown' values count per column:\")\n",
    "for col in categorical_cols:\n",
    "    unknown_count = (df_clean[col] == 'unknown').sum()\n",
    "    if unknown_count > 0:\n",
    "        print(f\"{col}: {unknown_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6dc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e474c3",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5605a2",
   "metadata": {},
   "source": [
    "### 3.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for target variable\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "target_counts = df_clean['y'].value_counts()\n",
    "axes[0].pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', \n",
    "            startangle=90, colors=['#ff9999', '#66b3ff'])\n",
    "axes[0].set_title('Target Variable Distribution (y)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df_clean, x='y', palette='Set2', ax=axes[1])\n",
    "axes[1].set_title('Target Variable Count', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Subscribed to Term Deposit')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Add count labels\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(df_clean['y'].value_counts())\n",
    "print(f\"\\nClass Balance Ratio: {df_clean['y'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d720f",
   "metadata": {},
   "source": [
    "### 3.2 Categorical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plots for key categorical variables\n",
    "cat_features = ['job', 'marital', 'education', 'contact', 'month', 'day_of_week']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(cat_features):\n",
    "    order = df_clean[col].value_counts().index\n",
    "    sns.countplot(data=df_clean, y=col, order=order, palette='viridis', ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribution of {col.upper()}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Count')\n",
    "    axes[idx].set_ylabel(col.capitalize())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8585b82",
   "metadata": {},
   "source": [
    "### 3.3 Numerical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ed128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and KDE for age and duration\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 0].hist(df_clean['age'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Age Distribution (Histogram)', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Age KDE\n",
    "df_clean['age'].plot(kind='kde', ax=axes[0, 1], color='blue', linewidth=2)\n",
    "axes[0, 1].set_title('Age Distribution (KDE)', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "\n",
    "# Duration distribution\n",
    "axes[1, 0].hist(df_clean['duration'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Call Duration Distribution (Histogram)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Duration (seconds)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Duration KDE\n",
    "df_clean['duration'].plot(kind='kde', ax=axes[1, 1], color='red', linewidth=2)\n",
    "axes[1, 1].set_title('Call Duration Distribution (KDE)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Duration (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44b4da",
   "metadata": {},
   "source": [
    "### 3.4 Outlier Detection with Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846075e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical features to detect outliers\n",
    "numeric_features = ['age', 'duration', 'campaign', 'pdays', 'previous', \n",
    "                    'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_features):\n",
    "    sns.boxplot(data=df_clean, y=col, color='lightgreen', ax=axes[idx])\n",
    "    axes[idx].set_title(f'Box Plot: {col.upper()}', fontweight='bold')\n",
    "    axes[idx].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02da16",
   "metadata": {},
   "source": [
    "### 3.5 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95785b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical variables\n",
    "# First, encode target variable for correlation analysis\n",
    "df_corr = df_clean.copy()\n",
    "df_corr['y_encoded'] = df_corr['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Select numeric columns + encoded target\n",
    "numeric_cols = df_corr.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df_corr[numeric_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            cbar_kws={'label': 'Correlation Coefficient'},\n",
    "            linewidths=0.5, square=True)\n",
    "plt.title('Correlation Heatmap of Numerical Features', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top correlations with target\n",
    "print(\"\\nTop correlations with target variable (y):\")\n",
    "target_corr = corr_matrix['y_encoded'].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5ec7f",
   "metadata": {},
   "source": [
    "### 3.6 Pair Plot (Key Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940340a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot for selected key numerical features\n",
    "key_features = ['age', 'duration', 'campaign', 'euribor3m', 'y']\n",
    "sample_df = df_clean[key_features].sample(n=min(1000, len(df_clean)), random_state=42)\n",
    "\n",
    "sns.pairplot(sample_df, hue='y', palette='Set1', diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('Pair Plot: Key Numerical Features', y=1.02, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5fcceb",
   "metadata": {},
   "source": [
    "### 3.7 Subscription Rate by Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2eeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar charts: subscription rate vs categorical features\n",
    "categorical_features = ['job', 'marital', 'education', 'contact']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    # Create cross-tabulation\n",
    "    ct = pd.crosstab(df_clean[col], df_clean['y'], normalize='index') * 100\n",
    "    ct.plot(kind='bar', ax=axes[idx], color=['#ff9999', '#66b3ff'], width=0.8)\n",
    "    axes[idx].set_title(f'Subscription Rate by {col.upper()}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col.capitalize())\n",
    "    axes[idx].set_ylabel('Percentage (%)')\n",
    "    axes[idx].legend(title='Subscribed', labels=['No', 'Yes'])\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43af083",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5921d5a",
   "metadata": {},
   "source": [
    "### 4.1 Encode Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable: yes=1, no=0\n",
    "df_clean['y'] = df_clean['y'].map({'yes': 1, 'no': 0})\n",
    "print(\"Target variable encoded:\")\n",
    "print(df_clean['y'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd170c",
   "metadata": {},
   "source": [
    "### 4.2 Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71adc3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df_clean.drop('y', axis=1)\n",
    "y = df_clean['y']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed3d77",
   "metadata": {},
   "source": [
    "### 4.3 Identify Categorical and Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680de3d5",
   "metadata": {},
   "source": [
    "### 4.4 Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a65d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "# Numerical: StandardScaler\n",
    "# Categorical: OneHotEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "print(\"✅ Preprocessing pipeline created!\")\n",
    "print(\"   - Numerical features: StandardScaler\")\n",
    "print(\"   - Categorical features: OneHotEncoder (drop_first=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c25c88",
   "metadata": {},
   "source": [
    "### 4.5 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "print(f\"\\nTarget distribution in training set:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\nTarget distribution in testing set:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd923a0",
   "metadata": {},
   "source": [
    "## 5. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45312622",
   "metadata": {},
   "source": [
    "### 5.1 Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76829e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "}\n",
    "\n",
    "print(\"Models to train:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207438a",
   "metadata": {},
   "source": [
    "### 5.2 Train Models and Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e676c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and collect metrics\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Create pipeline: preprocessor + model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    # Store trained model\n",
    "    trained_models[model_name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ All models trained successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397e9c2",
   "metadata": {},
   "source": [
    "### 5.3 Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n📊 Model Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    results_df_sorted = results_df.sort_values(metric, ascending=True)\n",
    "    bars = ax.barh(results_df_sorted['Model'], results_df_sorted[metric], color='steelblue')\n",
    "    ax.set_xlabel(metric, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "# Remove the extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0625c7",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285e966",
   "metadata": {},
   "source": [
    "### 6.1 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, model_data) in enumerate(trained_models.items()):\n",
    "    cm = confusion_matrix(y_test, model_data['y_pred'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
    "    disp.plot(ax=axes[idx], cmap='Blues', values_format='d')\n",
    "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold', fontsize=12)\n",
    "    axes[idx].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a6287",
   "metadata": {},
   "source": [
    "### 6.2 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "for idx, (model_name, model_data) in enumerate(trained_models.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, model_data['y_pred_proba'])\n",
    "    roc_auc = roc_auc_score(y_test, model_data['y_pred_proba'])\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[idx], lw=2, \n",
    "             label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a58e2",
   "metadata": {},
   "source": [
    "### 6.3 Feature Importance (Random Forest & XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26768e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "def get_feature_names(preprocessor, X):\n",
    "    # Get feature names from ColumnTransformer\n",
    "    feature_names = []\n",
    "    \n",
    "    # Numerical features\n",
    "    num_features = preprocessor.transformers_[0][2]\n",
    "    feature_names.extend(num_features)\n",
    "    \n",
    "    # Categorical features (one-hot encoded)\n",
    "    cat_transformer = preprocessor.transformers_[1][1]\n",
    "    cat_features = preprocessor.transformers_[1][2]\n",
    "    cat_feature_names = cat_transformer.get_feature_names_out(cat_features)\n",
    "    feature_names.extend(cat_feature_names)\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "# Get feature names\n",
    "feature_names = get_feature_names(preprocessor, X_train)\n",
    "print(f\"Total features after encoding: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "rf_model = trained_models['Random Forest']['pipeline'].named_steps['classifier']\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=rf_importance, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Top 15 Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontweight='bold')\n",
    "plt.ylabel('Feature', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Top 15 Important Features (Random Forest):\")\n",
    "print(rf_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c28035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for XGBoost\n",
    "xgb_model = trained_models['XGBoost']['pipeline'].named_steps['classifier']\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=xgb_importance, x='Importance', y='Feature', palette='plasma')\n",
    "plt.title('Top 15 Feature Importance - XGBoost', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontweight='bold')\n",
    "plt.ylabel('Feature', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Top 15 Important Features (XGBoost):\")\n",
    "print(xgb_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92f887",
   "metadata": {},
   "source": [
    "## 7. Insights & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe92ad9",
   "metadata": {},
   "source": [
    "### 7.1 Key Insights from Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=\"*80)\n",
    "print(\"🔍 KEY INSIGHTS FROM BANK MARKETING CAMPAIGN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1️⃣ DATA OVERVIEW:\")\n",
    "print(f\"   - Total Records: {len(df_clean):,}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "print(f\"   - Class Imbalance: {(y==0).sum():,} No vs {(y==1).sum():,} Yes\")\n",
    "print(f\"   - Subscription Rate: {(y==1).sum()/len(y)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n2️⃣ BEST PERFORMING MODEL:\")\n",
    "best_model = results_df.iloc[0]\n",
    "print(f\"   - Model: {best_model['Model']}\")\n",
    "print(f\"   - ROC-AUC: {best_model['ROC-AUC']:.4f}\")\n",
    "print(f\"   - Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"   - F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\n3️⃣ TOP PREDICTIVE FEATURES (based on Random Forest):\")\n",
    "top_features = rf_importance.head(5)['Feature'].tolist()\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "print(\"\\n4️⃣ BUSINESS RECOMMENDATIONS:\")\n",
    "print(\"   ✅ Focus on call duration - longer conversations increase subscription likelihood\")\n",
    "print(\"   ✅ Target customers based on economic indicators (euribor3m, emp.var.rate)\")\n",
    "print(\"   ✅ Prioritize cellular contact method over telephone\")\n",
    "print(\"   ✅ Optimize campaign timing - certain months show higher success rates\")\n",
    "print(\"   ✅ Consider customer demographics (age, job, education) for targeted campaigns\")\n",
    "\n",
    "print(\"\\n5️⃣ MODEL DEPLOYMENT STRATEGY:\")\n",
    "print(f\"   - Deploy {best_model['Model']} for production predictions\")\n",
    "print(\"   - Use probability scores to prioritize high-likelihood customers\")\n",
    "print(\"   - Implement A/B testing to validate model performance in real campaigns\")\n",
    "print(\"   - Regular model retraining with new campaign data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549f424",
   "metadata": {},
   "source": [
    "### 7.2 Marketing Strategy Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze high-likelihood customer segments\n",
    "print(\"📈 CUSTOMER SEGMENTATION FOR TARGETED MARKETING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use best model to get probabilities for all data\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_pipeline = trained_models[best_model_name]['pipeline']\n",
    "\n",
    "# Get predictions for entire dataset\n",
    "all_probas = best_pipeline.predict_proba(X)[:, 1]\n",
    "df_with_scores = df_clean.copy()\n",
    "df_with_scores['subscription_probability'] = all_probas\n",
    "\n",
    "# Segment customers by probability\n",
    "df_with_scores['segment'] = pd.cut(df_with_scores['subscription_probability'], \n",
    "                                     bins=[0, 0.3, 0.6, 1.0],\n",
    "                                     labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"\\nCustomer Segments by Subscription Probability:\")\n",
    "print(df_with_scores['segment'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n💡 RECOMMENDED ACTIONS BY SEGMENT:\")\n",
    "print(\"\\n🔴 HIGH PROBABILITY (>60%):\")\n",
    "high_prob = df_with_scores[df_with_scores['segment'] == 'High']\n",
    "print(f\"   - Count: {len(high_prob):,} customers\")\n",
    "print(\"   - Action: Priority outreach with personalized offers\")\n",
    "print(\"   - Expected conversion rate: High (>60%)\")\n",
    "\n",
    "print(\"\\n🟡 MEDIUM PROBABILITY (30-60%):\")\n",
    "med_prob = df_with_scores[df_with_scores['segment'] == 'Medium']\n",
    "print(f\"   - Count: {len(med_prob):,} customers\")\n",
    "print(\"   - Action: Targeted campaigns with incentives\")\n",
    "print(\"   - Expected conversion rate: Moderate (30-60%)\")\n",
    "\n",
    "print(\"\\n🟢 LOW PROBABILITY (<30%):\")\n",
    "low_prob = df_with_scores[df_with_scores['segment'] == 'Low']\n",
    "print(f\"   - Count: {len(low_prob):,} customers\")\n",
    "print(\"   - Action: Minimal contact, focus on brand awareness\")\n",
    "print(\"   - Expected conversion rate: Low (<30%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cea85",
   "metadata": {},
   "source": [
    "### 7.3 Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📋 DELIVERABLES:\")\n",
    "print(\"   ✓ Comprehensive EDA with 10+ visualizations\")\n",
    "print(\"   ✓ Feature engineering and preprocessing pipeline\")\n",
    "print(\"   ✓ 4 trained classification models\")\n",
    "print(\"   ✓ Model evaluation with confusion matrices and ROC curves\")\n",
    "print(\"   ✓ Feature importance analysis\")\n",
    "print(\"   ✓ Business insights and marketing recommendations\")\n",
    "print(\"   ✓ Customer segmentation strategy\")\n",
    "\n",
    "print(\"\\n🎯 NEXT STEPS:\")\n",
    "print(\"   1. Deploy best model to production environment\")\n",
    "print(\"   2. Implement real-time scoring API\")\n",
    "print(\"   3. Set up monitoring and model drift detection\")\n",
    "print(\"   4. A/B test model predictions vs. current strategy\")\n",
    "print(\"   5. Collect feedback and retrain model quarterly\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Thank you for using this ML pipeline! 🚀\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
