{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b7bd86",
   "metadata": {},
   "source": [
    "# Predicting Bank Campaign Success\n",
    "\n",
    "**Objective:** Build a binary classification model to predict whether a customer will subscribe to a term deposit (`y`) based on demographic, financial, and campaign-related features.\n",
    "\n",
    "**Dataset:** Bank Marketing Campaign (semicolon-separated CSV)\n",
    "\n",
    "**Target Variable:** `y` (yes/no) â€” Term deposit subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03cb970",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff5d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b52da1",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b367f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (semicolon-separated)\n",
    "df = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64023f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Duplicate Rows:\")\n",
    "print(f\"Number of duplicates: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d24104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle duplicates if any\n",
    "df_clean = df.drop_duplicates()\n",
    "print(f\"Shape after removing duplicates: {df_clean.shape}\")\n",
    "\n",
    "# Check for 'unknown' values in categorical columns\n",
    "categorical_cols = df_clean.select_dtypes(include='object').columns\n",
    "print(\"\\n'Unknown' values count per column:\")\n",
    "for col in categorical_cols:\n",
    "    unknown_count = (df_clean[col] == 'unknown').sum()\n",
    "    if unknown_count > 0:\n",
    "        print(f\"{col}: {unknown_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6dc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e474c3",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5605a2",
   "metadata": {},
   "source": [
    "### 3.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for target variable\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "target_counts = df_clean['y'].value_counts()\n",
    "axes[0].pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', \n",
    "            startangle=90, colors=['#ff9999', '#66b3ff'])\n",
    "axes[0].set_title('Target Variable Distribution (y)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df_clean, x='y', palette='Set2', ax=axes[1])\n",
    "axes[1].set_title('Target Variable Count', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Subscribed to Term Deposit')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Add count labels\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(df_clean['y'].value_counts())\n",
    "print(f\"\\nClass Balance Ratio: {df_clean['y'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d720f",
   "metadata": {},
   "source": [
    "### 3.2 Categorical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plots for key categorical variables\n",
    "cat_features = ['job', 'marital', 'education', 'contact', 'month', 'day_of_week']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(cat_features):\n",
    "    order = df_clean[col].value_counts().index\n",
    "    sns.countplot(data=df_clean, y=col, order=order, palette='viridis', ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribution of {col.upper()}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Count')\n",
    "    axes[idx].set_ylabel(col.capitalize())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8585b82",
   "metadata": {},
   "source": [
    "### 3.3 Numerical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ed128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and KDE for age and duration\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 0].hist(df_clean['age'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Age Distribution (Histogram)', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Age KDE\n",
    "df_clean['age'].plot(kind='kde', ax=axes[0, 1], color='blue', linewidth=2)\n",
    "axes[0, 1].set_title('Age Distribution (KDE)', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "\n",
    "# Duration distribution\n",
    "axes[1, 0].hist(df_clean['duration'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Call Duration Distribution (Histogram)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Duration (seconds)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Duration KDE\n",
    "df_clean['duration'].plot(kind='kde', ax=axes[1, 1], color='red', linewidth=2)\n",
    "axes[1, 1].set_title('Call Duration Distribution (KDE)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Duration (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44b4da",
   "metadata": {},
   "source": [
    "### 3.4 Outlier Detection with Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846075e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical features to detect outliers\n",
    "numeric_features = ['age', 'duration', 'campaign', 'pdays', 'previous', \n",
    "                    'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_features):\n",
    "    sns.boxplot(data=df_clean, y=col, color='lightgreen', ax=axes[idx])\n",
    "    axes[idx].set_title(f'Box Plot: {col.upper()}', fontweight='bold')\n",
    "    axes[idx].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02da16",
   "metadata": {},
   "source": [
    "### 3.5 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95785b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical variables\n",
    "# First, encode target variable for correlation analysis\n",
    "df_corr = df_clean.copy()\n",
    "df_corr['y_encoded'] = df_corr['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Select numeric columns + encoded target\n",
    "numeric_cols = df_corr.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df_corr[numeric_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            cbar_kws={'label': 'Correlation Coefficient'},\n",
    "            linewidths=0.5, square=True)\n",
    "plt.title('Correlation Heatmap of Numerical Features', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top correlations with target\n",
    "print(\"\\nTop correlations with target variable (y):\")\n",
    "target_corr = corr_matrix['y_encoded'].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5ec7f",
   "metadata": {},
   "source": [
    "### 3.6 Pair Plot (Key Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940340a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot for selected key numerical features\n",
    "key_features = ['age', 'duration', 'campaign', 'euribor3m', 'y']\n",
    "sample_df = df_clean[key_features].sample(n=min(1000, len(df_clean)), random_state=42)\n",
    "\n",
    "sns.pairplot(sample_df, hue='y', palette='Set1', diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('Pair Plot: Key Numerical Features', y=1.02, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5fcceb",
   "metadata": {},
   "source": [
    "### 3.7 Subscription Rate by Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2eeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar charts: subscription rate vs categorical features\n",
    "categorical_features = ['job', 'marital', 'education', 'contact']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    # Create cross-tabulation\n",
    "    ct = pd.crosstab(df_clean[col], df_clean['y'], normalize='index') * 100\n",
    "    ct.plot(kind='bar', ax=axes[idx], color=['#ff9999', '#66b3ff'], width=0.8)\n",
    "    axes[idx].set_title(f'Subscription Rate by {col.upper()}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col.capitalize())\n",
    "    axes[idx].set_ylabel('Percentage (%)')\n",
    "    axes[idx].legend(title='Subscribed', labels=['No', 'Yes'])\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43af083",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5921d5a",
   "metadata": {},
   "source": [
    "### 4.1 Encode Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable: yes=1, no=0\n",
    "df_clean['y'] = df_clean['y'].map({'yes': 1, 'no': 0})\n",
    "print(\"Target variable encoded:\")\n",
    "print(df_clean['y'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd170c",
   "metadata": {},
   "source": [
    "### 4.2 Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71adc3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df_clean.drop('y', axis=1)\n",
    "y = df_clean['y']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed3d77",
   "metadata": {},
   "source": [
    "### 4.3 Identify Categorical and Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680de3d5",
   "metadata": {},
   "source": [
    "### 4.4 Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a65d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "# Numerical: StandardScaler\n",
    "# Categorical: OneHotEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "print(\"âœ… Preprocessing pipeline created!\")\n",
    "print(\"   - Numerical features: StandardScaler\")\n",
    "print(\"   - Categorical features: OneHotEncoder (drop_first=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c25c88",
   "metadata": {},
   "source": [
    "### 4.5 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "print(f\"\\nTarget distribution in training set:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\nTarget distribution in testing set:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd923a0",
   "metadata": {},
   "source": [
    "## 5. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45312622",
   "metadata": {},
   "source": [
    "### 5.1 Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76829e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "}\n",
    "\n",
    "print(\"Models to train:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207438a",
   "metadata": {},
   "source": [
    "### 5.2 Train Models and Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e676c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and collect metrics\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Create pipeline: preprocessor + model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    # Store trained model\n",
    "    trained_models[model_name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… All models trained successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397e9c2",
   "metadata": {},
   "source": [
    "### 5.3 Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nðŸ“Š Model Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    results_df_sorted = results_df.sort_values(metric, ascending=True)\n",
    "    bars = ax.barh(results_df_sorted['Model'], results_df_sorted[metric], color='steelblue')\n",
    "    ax.set_xlabel(metric, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "# Remove the extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0625c7",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285e966",
   "metadata": {},
   "source": [
    "### 6.1 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, model_data) in enumerate(trained_models.items()):\n",
    "    cm = confusion_matrix(y_test, model_data['y_pred'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
    "    disp.plot(ax=axes[idx], cmap='Blues', values_format='d')\n",
    "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold', fontsize=12)\n",
    "    axes[idx].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a6287",
   "metadata": {},
   "source": [
    "### 6.2 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "for idx, (model_name, model_data) in enumerate(trained_models.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, model_data['y_pred_proba'])\n",
    "    roc_auc = roc_auc_score(y_test, model_data['y_pred_proba'])\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[idx], lw=2, \n",
    "             label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a58e2",
   "metadata": {},
   "source": [
    "### 6.3 Feature Importance (Random Forest & XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26768e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "def get_feature_names(preprocessor, X):\n",
    "    # Get feature names from ColumnTransformer\n",
    "    feature_names = []\n",
    "    \n",
    "    # Numerical features\n",
    "    num_features = preprocessor.transformers_[0][2]\n",
    "    feature_names.extend(num_features)\n",
    "    \n",
    "    # Categorical features (one-hot encoded)\n",
    "    cat_transformer = preprocessor.transformers_[1][1]\n",
    "    cat_features = preprocessor.transformers_[1][2]\n",
    "    cat_feature_names = cat_transformer.get_feature_names_out(cat_features)\n",
    "    feature_names.extend(cat_feature_names)\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "# Get feature names\n",
    "feature_names = get_feature_names(preprocessor, X_train)\n",
    "print(f\"Total features after encoding: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "rf_model = trained_models['Random Forest']['pipeline'].named_steps['classifier']\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=rf_importance, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Top 15 Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontweight='bold')\n",
    "plt.ylabel('Feature', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Top 15 Important Features (Random Forest):\")\n",
    "print(rf_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c28035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for XGBoost\n",
    "xgb_model = trained_models['XGBoost']['pipeline'].named_steps['classifier']\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=xgb_importance, x='Importance', y='Feature', palette='plasma')\n",
    "plt.title('Top 15 Feature Importance - XGBoost', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontweight='bold')\n",
    "plt.ylabel('Feature', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Top 15 Important Features (XGBoost):\")\n",
    "print(xgb_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92f887",
   "metadata": {},
   "source": [
    "## 7. Insights & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe92ad9",
   "metadata": {},
   "source": [
    "### 7.1 Key Insights from Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ” KEY INSIGHTS FROM BANK MARKETING CAMPAIGN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ DATA OVERVIEW:\")\n",
    "print(f\"   - Total Records: {len(df_clean):,}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "print(f\"   - Class Imbalance: {(y==0).sum():,} No vs {(y==1).sum():,} Yes\")\n",
    "print(f\"   - Subscription Rate: {(y==1).sum()/len(y)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ BEST PERFORMING MODEL:\")\n",
    "best_model = results_df.iloc[0]\n",
    "print(f\"   - Model: {best_model['Model']}\")\n",
    "print(f\"   - ROC-AUC: {best_model['ROC-AUC']:.4f}\")\n",
    "print(f\"   - Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"   - F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ TOP PREDICTIVE FEATURES (based on Random Forest):\")\n",
    "top_features = rf_importance.head(5)['Feature'].tolist()\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ BUSINESS RECOMMENDATIONS:\")\n",
    "print(\"   âœ… Focus on call duration - longer conversations increase subscription likelihood\")\n",
    "print(\"   âœ… Target customers based on economic indicators (euribor3m, emp.var.rate)\")\n",
    "print(\"   âœ… Prioritize cellular contact method over telephone\")\n",
    "print(\"   âœ… Optimize campaign timing - certain months show higher success rates\")\n",
    "print(\"   âœ… Consider customer demographics (age, job, education) for targeted campaigns\")\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ MODEL DEPLOYMENT STRATEGY:\")\n",
    "print(f\"   - Deploy {best_model['Model']} for production predictions\")\n",
    "print(\"   - Use probability scores to prioritize high-likelihood customers\")\n",
    "print(\"   - Implement A/B testing to validate model performance in real campaigns\")\n",
    "print(\"   - Regular model retraining with new campaign data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549f424",
   "metadata": {},
   "source": [
    "### 7.2 Marketing Strategy Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze high-likelihood customer segments\n",
    "print(\"ðŸ“ˆ CUSTOMER SEGMENTATION FOR TARGETED MARKETING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use best model to get probabilities for all data\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_pipeline = trained_models[best_model_name]['pipeline']\n",
    "\n",
    "# Get predictions for entire dataset\n",
    "all_probas = best_pipeline.predict_proba(X)[:, 1]\n",
    "df_with_scores = df_clean.copy()\n",
    "df_with_scores['subscription_probability'] = all_probas\n",
    "\n",
    "# Segment customers by probability\n",
    "df_with_scores['segment'] = pd.cut(df_with_scores['subscription_probability'], \n",
    "                                     bins=[0, 0.3, 0.6, 1.0],\n",
    "                                     labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"\\nCustomer Segments by Subscription Probability:\")\n",
    "print(df_with_scores['segment'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nðŸ’¡ RECOMMENDED ACTIONS BY SEGMENT:\")\n",
    "print(\"\\nðŸ”´ HIGH PROBABILITY (>60%):\")\n",
    "high_prob = df_with_scores[df_with_scores['segment'] == 'High']\n",
    "print(f\"   - Count: {len(high_prob):,} customers\")\n",
    "print(\"   - Action: Priority outreach with personalized offers\")\n",
    "print(\"   - Expected conversion rate: High (>60%)\")\n",
    "\n",
    "print(\"\\nðŸŸ¡ MEDIUM PROBABILITY (30-60%):\")\n",
    "med_prob = df_with_scores[df_with_scores['segment'] == 'Medium']\n",
    "print(f\"   - Count: {len(med_prob):,} customers\")\n",
    "print(\"   - Action: Targeted campaigns with incentives\")\n",
    "print(\"   - Expected conversion rate: Moderate (30-60%)\")\n",
    "\n",
    "print(\"\\nðŸŸ¢ LOW PROBABILITY (<30%):\")\n",
    "low_prob = df_with_scores[df_with_scores['segment'] == 'Low']\n",
    "print(f\"   - Count: {len(low_prob):,} customers\")\n",
    "print(\"   - Action: Minimal contact, focus on brand awareness\")\n",
    "print(\"   - Expected conversion rate: Low (<30%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cea85",
   "metadata": {},
   "source": [
    "### 7.3 Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“‹ DELIVERABLES:\")\n",
    "print(\"   âœ“ Comprehensive EDA with 10+ visualizations\")\n",
    "print(\"   âœ“ Feature engineering and preprocessing pipeline\")\n",
    "print(\"   âœ“ 4 trained classification models\")\n",
    "print(\"   âœ“ Model evaluation with confusion matrices and ROC curves\")\n",
    "print(\"   âœ“ Feature importance analysis\")\n",
    "print(\"   âœ“ Business insights and marketing recommendations\")\n",
    "print(\"   âœ“ Customer segmentation strategy\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ NEXT STEPS:\")\n",
    "print(\"   1. Deploy best model to production environment\")\n",
    "print(\"   2. Implement real-time scoring API\")\n",
    "print(\"   3. Set up monitoring and model drift detection\")\n",
    "print(\"   4. A/B test model predictions vs. current strategy\")\n",
    "print(\"   5. Collect feedback and retrain model quarterly\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Thank you for using this ML pipeline! ðŸš€\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
